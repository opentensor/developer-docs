
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>bittensor.core.metagraph &#8212; Bittensor SDK Docs  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=fd3f3429" />
    <link rel="stylesheet" type="text/css" href="../../../_static/bittensor-custom.css?v=c1dfe055" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/bittensor/core/metagraph';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/logo.svg" class="logo__image only-light" alt="Bittensor SDK Docs  documentation - Home"/>
    <img src="../../../_static/logo-dark-mode.svg" class="logo__image only-dark pst-js-only" alt="Bittensor SDK Docs  documentation - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Table of Contents</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../autoapi/index.html">API Reference</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../autoapi/bittensor/index.html">bittensor</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../autoapi/bittensor/core/index.html">bittensor.core</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/async_subtensor/index.html">bittensor.core.async_subtensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/axon/index.html">bittensor.core.axon</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/chain_data/index.html">bittensor.core.chain_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/config/index.html">bittensor.core.config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/dendrite/index.html">bittensor.core.dendrite</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/errors/index.html">bittensor.core.errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/extrinsics/index.html">bittensor.core.extrinsics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/metagraph/index.html">bittensor.core.metagraph</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/settings/index.html">bittensor.core.settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/stream/index.html">bittensor.core.stream</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/subtensor/index.html">bittensor.core.subtensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/synapse/index.html">bittensor.core.synapse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/tensor/index.html">bittensor.core.tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/threadpool/index.html">bittensor.core.threadpool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/core/types/index.html">bittensor.core.types</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../autoapi/bittensor/utils/index.html">bittensor.utils</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/axon_utils/index.html">bittensor.utils.axon_utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/balance/index.html">bittensor.utils.balance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/btlogging/index.html">bittensor.utils.btlogging</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/delegates_details/index.html">bittensor.utils.delegates_details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/deprecated/index.html">bittensor.utils.deprecated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/formatting/index.html">bittensor.utils.formatting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/mock/index.html">bittensor.utils.mock</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/networking/index.html">bittensor.utils.networking</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/registration/index.html">bittensor.utils.registration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/subnets/index.html">bittensor.utils.subnets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/substrate_utils/index.html">bittensor.utils.substrate_utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/version/index.html">bittensor.utils.version</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../autoapi/bittensor/utils/weight_utils/index.html">bittensor.utils.weight_utils</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/opentensor/btcli" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/opentensor/btcli/issues/new?title=Issue%20on%20page%20%2F_modules/bittensor/core/metagraph.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for bittensor.core.metagraph</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">typing</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">listdir</span>
<span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <span class="n">join</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.typing</span> <span class="kn">import</span> <span class="n">NDArray</span>

<span class="kn">from</span> <span class="nn">bittensor.utils.btlogging</span> <span class="kn">import</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">bittensor.utils.registration</span> <span class="kn">import</span> <span class="n">torch</span><span class="p">,</span> <span class="n">use_torch</span>
<span class="kn">from</span> <span class="nn">bittensor.utils.weight_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">convert_weight_uids_and_vals_to_tensor</span><span class="p">,</span>
    <span class="n">convert_bond_uids_and_vals_to_tensor</span><span class="p">,</span>
    <span class="n">convert_root_weight_uids_and_vals_to_tensor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">settings</span>
<span class="kn">from</span> <span class="nn">.chain_data</span> <span class="kn">import</span> <span class="n">AxonInfo</span>

<span class="c1"># For annotation purposes</span>
<span class="k">if</span> <span class="n">typing</span><span class="o">.</span><span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">bittensor.core.subtensor</span> <span class="kn">import</span> <span class="n">Subtensor</span>


<span class="n">METAGRAPH_STATE_DICT_NDARRAY_KEYS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;version&quot;</span><span class="p">,</span>
    <span class="s2">&quot;n&quot;</span><span class="p">,</span>
    <span class="s2">&quot;block&quot;</span><span class="p">,</span>
    <span class="s2">&quot;stake&quot;</span><span class="p">,</span>
    <span class="s2">&quot;total_stake&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ranks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;trust&quot;</span><span class="p">,</span>
    <span class="s2">&quot;consensus&quot;</span><span class="p">,</span>
    <span class="s2">&quot;validator_trust&quot;</span><span class="p">,</span>
    <span class="s2">&quot;incentive&quot;</span><span class="p">,</span>
    <span class="s2">&quot;emission&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dividends&quot;</span><span class="p">,</span>
    <span class="s2">&quot;active&quot;</span><span class="p">,</span>
    <span class="s2">&quot;last_update&quot;</span><span class="p">,</span>
    <span class="s2">&quot;validator_permit&quot;</span><span class="p">,</span>
    <span class="s2">&quot;uids&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="sd">&quot;&quot;&quot;List of keys for the metagraph state dictionary used in NDArray serialization.</span>

<span class="sd">This list defines the set of keys expected in the metagraph&#39;s state dictionary when serializing and deserializing NumPy ndarray objects. Each key corresponds to a specific attribute or metric associated with the nodes in the metagraph.</span>

<span class="sd">- **version** (`str`): The version identifier of the metagraph state.</span>
<span class="sd">- **n** (`int`): The total number of nodes in the metagraph.</span>
<span class="sd">- **block** (`int`): The current block number in the blockchain or ledger.</span>
<span class="sd">- **stake** (`ndarray`): An array representing the stake of each node.</span>
<span class="sd">- **total_stake** (`float`): The sum of all individual stakes in the metagraph.</span>
<span class="sd">- **ranks** (`ndarray`): An array of rank scores assigned to each node.</span>
<span class="sd">- **trust** (`ndarray`): An array of trust scores for the nodes.</span>
<span class="sd">- **consensus** (`ndarray`): An array indicating consensus levels among nodes.</span>
<span class="sd">- **validator_trust** (`ndarray`): Trust scores specific to validator nodes.</span>
<span class="sd">- **incentive** (`ndarray`): Incentive values allocated to nodes.</span>
<span class="sd">- **emission** (`float`): The rate of emission for new tokens or units.</span>
<span class="sd">- **dividends** (`ndarray`): Dividend amounts distributed to nodes.</span>
<span class="sd">- **active** (`ndarray`): Boolean array indicating active (`True`) or inactive (`False`) nodes.</span>
<span class="sd">- **last_update** (`int`): Timestamp of the last state update.</span>
<span class="sd">- **validator_permit** (`ndarray`): Boolean array indicating nodes permitted to validate.</span>
<span class="sd">- **uids** (`ndarray`): Unique identifiers for each node in the metagraph.</span>
<span class="sd">&quot;&quot;&quot;</span>


<div class="viewcode-block" id="get_save_dir">
<a class="viewcode-back" href="../../../autoapi/bittensor/core/metagraph/index.html#bittensor.core.metagraph.get_save_dir">[docs]</a>
<span class="k">def</span> <span class="nf">get_save_dir</span><span class="p">(</span><span class="n">network</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">netuid</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a directory path given ``network`` and ``netuid`` inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        network (str): Network name.</span>
<span class="sd">        netuid (int): Network UID.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: Directory path.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="s2">&quot;~&quot;</span><span class="p">,</span>
            <span class="s2">&quot;.bittensor&quot;</span><span class="p">,</span>
            <span class="s2">&quot;metagraphs&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;network-</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">network</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;netuid-</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">netuid</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span></div>



<div class="viewcode-block" id="latest_block_path">
<a class="viewcode-back" href="../../../autoapi/bittensor/core/metagraph/index.html#bittensor.core.metagraph.latest_block_path">[docs]</a>
<span class="k">def</span> <span class="nf">latest_block_path</span><span class="p">(</span><span class="n">dir_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the latest block path from the provided directory path.</span>

<span class="sd">    Args:</span>
<span class="sd">        dir_path (str): Directory path.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: Latest block path.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">latest_block</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">latest_file_full_path</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">listdir</span><span class="p">(</span><span class="n">dir_path</span><span class="p">):</span>
        <span class="n">full_path_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">dir_path</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">block_number</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">filename</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">block_number</span> <span class="o">&gt;</span> <span class="n">latest_block</span><span class="p">:</span>
                <span class="n">latest_block</span> <span class="o">=</span> <span class="n">block_number</span>
                <span class="n">latest_file_full_path</span> <span class="o">=</span> <span class="n">full_path_filename</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">latest_file_full_path</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Metagraph not found at: </span><span class="si">{</span><span class="n">dir_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">latest_file_full_path</span></div>



<div class="viewcode-block" id="determine_chain_endpoint_and_network">
<a class="viewcode-back" href="../../../autoapi/bittensor/core/metagraph/index.html#bittensor.core.metagraph.determine_chain_endpoint_and_network">[docs]</a>
<span class="k">def</span> <span class="nf">determine_chain_endpoint_and_network</span><span class="p">(</span><span class="n">network</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine the chain endpoint and network name from the passed arg</span>

<span class="sd">    Args:</span>
<span class="sd">        network: The network name (e.g. &#39;finney&#39;, &#39;test&#39;) or</span>
<span class="sd">            chain endpoint (e.g. wss://entrypoint-finney.opentensor.ai:443)</span>

<span class="sd">    Returns:</span>
<span class="sd">        (network name, chain endpoint)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pathless_network</span> <span class="o">=</span> <span class="n">network</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">network</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">network</span>
    <span class="k">if</span> <span class="n">pathless_network</span> <span class="ow">in</span> <span class="n">settings</span><span class="o">.</span><span class="n">NETWORK_MAP</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pathless_network</span><span class="p">,</span> <span class="n">settings</span><span class="o">.</span><span class="n">NETWORK_MAP</span><span class="p">[</span><span class="n">pathless_network</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">pathless_network</span> <span class="ow">in</span> <span class="n">settings</span><span class="o">.</span><span class="n">REVERSE_NETWORK_MAP</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">settings</span><span class="o">.</span><span class="n">REVERSE_NETWORK_MAP</span><span class="p">[</span><span class="n">pathless_network</span><span class="p">],</span> <span class="n">pathless_network</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;unknown&quot;</span><span class="p">,</span> <span class="n">network</span></div>



<div class="viewcode-block" id="MetagraphMixin">
<a class="viewcode-back" href="../../../autoapi/bittensor/core/metagraph/index.html#bittensor.core.metagraph.MetagraphMixin">[docs]</a>
<span class="k">class</span> <span class="nc">MetagraphMixin</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The metagraph class is a core component of the Bittensor network, representing the neural graph that forms the backbone of the decentralized machine learning system.</span>

<span class="sd">    The metagraph is a dynamic representation of the network&#39;s state, capturing the interconnectedness and attributes of neurons (participants) in the Bittensor ecosystem. This class is not just a static structure but a live reflection of the network, constantly updated and synchronized with the state of the blockchain.</span>

<span class="sd">    In Bittensor, neurons are akin to nodes in a distributed system, each contributing computational resources and participating in the network&#39;s collective intelligence. The metagraph tracks various attributes of these neurons, such as stake, trust, and consensus, which are crucial for the network&#39;s incentive mechanisms and the Yuma Consensus algorithm as outlined in the `NeurIPS paper &lt;https://bittensor.com/pdfs/academia/NeurIPS_DAO_Workshop_2022_3_3.pdf&gt;`_. These attributes</span>
<span class="sd">    govern how neurons interact, how they are incentivized, and their roles within the network&#39;s</span>
<span class="sd">    decision-making processes.</span>

<span class="sd">    Args:</span>
<span class="sd">        netuid (int): A unique identifier that distinguishes between different instances or versions of the Bittensor network.</span>
<span class="sd">        network (str): The name of the network, signifying specific configurations or iterations within the Bittensor ecosystem.</span>
<span class="sd">        version (NDArray): The version number of the network, integral for tracking network updates.</span>
<span class="sd">        n (NDArray): The total number of neurons in the network, reflecting its size and complexity.</span>
<span class="sd">        block (NDArray): The current block number in the blockchain, crucial for synchronizing with the network&#39;s latest state.</span>
<span class="sd">        stake: Represents the cryptocurrency staked by neurons, impacting their influence and earnings within the network.</span>
<span class="sd">        total_stake: The cumulative stake across all neurons.</span>
<span class="sd">        ranks: Neuron rankings as per the Yuma Consensus algorithm, influencing their incentive distribution and network authority.</span>
<span class="sd">        trust: Scores indicating the reliability of neurons, mainly miners, within the network&#39;s operational context.</span>
<span class="sd">        consensus: Scores reflecting each neuron&#39;s alignment with the network&#39;s collective decisions.</span>
<span class="sd">        validator_trust: Trust scores for validator neurons, crucial for network security and validation.</span>
<span class="sd">        incentive: Rewards allocated to neurons, particularly miners, for their network contributions.</span>
<span class="sd">        emission: The rate at which rewards are distributed to neurons.</span>
<span class="sd">        dividends: Rewards received primarily by validators as part of the incentive mechanism.</span>
<span class="sd">        active: Status indicating whether a neuron is actively participating in the network.</span>
<span class="sd">        last_update: Timestamp of the latest update to a neuron&#39;s data.</span>
<span class="sd">        validator_permit: Indicates if a neuron is authorized to act as a validator.</span>
<span class="sd">        weights: Inter-neuronal weights set by each neuron, influencing network dynamics.</span>
<span class="sd">        bonds: Represents speculative investments by neurons in others, part of the reward mechanism.</span>
<span class="sd">        uids: Unique identifiers for each neuron, essential for network operations.</span>
<span class="sd">        axons (List): Details about each neuron&#39;s axon, critical for facilitating network communication.</span>

<span class="sd">    The metagraph plays a pivotal role in Bittensor&#39;s decentralized AI operations, influencing everything from data propagation to reward distribution. It embodies the principles of decentralized governance</span>
<span class="sd">    and collaborative intelligence, ensuring that the network remains adaptive, secure, and efficient.</span>

<span class="sd">    Example:</span>
<span class="sd">        Initializing the metagraph to represent the current state of the Bittensor network::</span>

<span class="sd">            from bittensor.core.metagraph import Metagraph</span>
<span class="sd">            metagraph = Metagraph(netuid=config.netuid, network=subtensor.network, sync=False)</span>

<span class="sd">        Synchronizing the metagraph with the network to reflect the latest state and neuron data::</span>

<span class="sd">            metagraph.sync(subtensor=subtensor)</span>

<span class="sd">        Accessing metagraph properties to inform network interactions and decisions::</span>

<span class="sd">            total_stake = metagraph.S</span>
<span class="sd">            neuron_ranks = metagraph.R</span>
<span class="sd">            neuron_incentives = metagraph.I</span>
<span class="sd">            axons = metagraph.axons</span>
<span class="sd">            neurons = metagraph.neurons</span>

<span class="sd">        Maintaining a local copy of hotkeys for querying and interacting with network entities::</span>

<span class="sd">            hotkeys = deepcopy(metagraph.hotkeys)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">netuid</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">network</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">version</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">NDArray</span><span class="p">]]</span>
    <span class="n">n</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">block</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">stake</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">total_stake</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">ranks</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">trust</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">consensus</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">validator_trust</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">incentive</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">emission</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">dividends</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">active</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">last_update</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">validator_permit</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">weights</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">bonds</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">uids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span>
    <span class="n">axons</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">AxonInfo</span><span class="p">]</span>
    <span class="n">chain_endpoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
    <span class="n">subtensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Subtensor&quot;</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Represents the stake of each neuron in the Bittensor network. Stake is an important concept in the</span>
<span class="sd">        Bittensor ecosystem, signifying the amount of network weight (or “stake”) each neuron holds,</span>
<span class="sd">        represented on a digital ledger. The stake influences a neuron&#39;s ability to contribute to and benefit</span>
<span class="sd">        from the network, playing a crucial role in the distribution of incentives and decision-making processes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            NDArray: A tensor representing the stake of each neuron in the network. Higher values signify a greater stake held by the respective neuron.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_stake</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">R</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Contains the ranks of neurons in the Bittensor network. Ranks are determined by the network based</span>
<span class="sd">        on each neuron&#39;s performance and contributions. Higher ranks typically indicate a greater level of</span>
<span class="sd">        contribution or performance by a neuron. These ranks are crucial in determining the distribution of</span>
<span class="sd">        incentives within the network, with higher-ranked neurons receiving more incentive.</span>

<span class="sd">        Returns:</span>
<span class="sd">            NDArray: A tensor where each element represents the rank of a neuron. Higher values indicate higher ranks within the network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ranks</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">I</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Incentive values of neurons represent the rewards they receive for their contributions to the network.</span>
<span class="sd">        The Bittensor network employs an incentive mechanism that rewards neurons based on their</span>
<span class="sd">        informational value, stake, and consensus with other peers. This ensures that the most valuable and</span>
<span class="sd">        trusted contributions are incentivized.</span>

<span class="sd">        Returns:</span>
<span class="sd">            NDArray: A tensor of incentive values, indicating the rewards or benefits accrued by each neuron based on their contributions and network consensus.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">incentive</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">E</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Denotes the emission values of neurons in the Bittensor network. Emissions refer to the distribution or</span>
<span class="sd">        release of rewards (often in the form of cryptocurrency) to neurons, typically based on their stake and</span>
<span class="sd">        performance. This mechanism is central to the network&#39;s incentive model, ensuring that active and</span>
<span class="sd">        contributing neurons are appropriately rewarded.</span>

<span class="sd">        Returns:</span>
<span class="sd">            NDArray: A tensor where each element represents the emission value for a neuron, indicating the amount of reward distributed to that neuron.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">emission</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">C</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Represents the consensus values of neurons in the Bittensor network. Consensus is a measure of how</span>
<span class="sd">        much a neuron&#39;s contributions are trusted and agreed upon by the majority of the network. It is</span>
<span class="sd">        calculated based on a staked weighted trust system, where the network leverages the collective</span>
<span class="sd">        judgment of all participating peers. Higher consensus values indicate that a neuron&#39;s contributions</span>
<span class="sd">        are more widely trusted and valued across the network.</span>

<span class="sd">        Returns:</span>
<span class="sd">            NDArray: A tensor of consensus values, where each element reflects the level of trust and agreement a neuron has achieved within the network.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">consensus</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Represents the trust values assigned to each neuron in the Bittensor network. Trust is a key metric that</span>
<span class="sd">        reflects the reliability and reputation of a neuron based on its past behavior and contributions. It is</span>
<span class="sd">        an essential aspect of the network&#39;s functioning, influencing decision-making processes and interactions</span>
<span class="sd">        between neurons.</span>

<span class="sd">        The trust matrix is inferred from the network&#39;s inter-peer weights, indicating the level of trust each neuron</span>
<span class="sd">        has in others. A higher value in the trust matrix suggests a stronger trust relationship between neurons.</span>

<span class="sd">        Returns:</span>
<span class="sd">            NDArray: A tensor of trust values, where each element represents the trust level of a neuron. Higher values denote a higher level of trust within the network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trust</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">Tv</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Contains the validator trust values of neurons in the Bittensor network. Validator trust is specifically</span>
<span class="sd">        associated with neurons that act as validators within the network. This specialized form of trust reflects</span>
<span class="sd">        the validators&#39; reliability and integrity in their role, which is crucial for maintaining the network&#39;s</span>
<span class="sd">        stability and security.</span>

<span class="sd">        Validator trust values are particularly important for the network&#39;s consensus and validation processes,</span>
<span class="sd">        determining the validators&#39; influence and responsibilities in these critical functions.</span>

<span class="sd">        Returns:</span>
<span class="sd">            NDArray: A tensor of validator trust values, specifically applicable to neurons serving as validators, where higher values denote greater trustworthiness in their validation roles.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">validator_trust</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">D</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Represents the dividends received by neurons in the Bittensor network. Dividends are a form of reward or</span>
<span class="sd">        distribution, typically given to neurons based on their stake, performance, and contribution to the network.</span>
<span class="sd">        They are an integral part of the network&#39;s incentive structure, encouraging active and beneficial participation.</span>

<span class="sd">        Returns:</span>
<span class="sd">            NDArray: A tensor of dividend values, where each element indicates the dividends received by a neuron, reflecting their share of network rewards.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dividends</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">B</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Bonds in the Bittensor network represent a speculative reward mechanism where neurons can accumulate</span>
<span class="sd">        bonds in other neurons. Bonds are akin to investments or stakes in other neurons, reflecting a belief in</span>
<span class="sd">        their future value or performance. This mechanism encourages correct weighting and collaboration</span>
<span class="sd">        among neurons while providing an additional layer of incentive.</span>

<span class="sd">        Returns:</span>
<span class="sd">            NDArray: A tensor representing the bonds held by each neuron, where each value signifies the proportion of bonds owned by one neuron in another.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bonds</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">W</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Represents the weights assigned to each neuron in the Bittensor network. In the context of Bittensor,</span>
<span class="sd">        weights are crucial for determining the influence and interaction between neurons. Each neuron is responsible</span>
<span class="sd">        for setting its weights, which are then recorded on a digital ledger. These weights are reflective of the</span>
<span class="sd">        neuron&#39;s assessment or judgment of other neurons in the network.</span>

<span class="sd">        The weight matrix :math:`W = [w_{ij}]` is a key component of the network&#39;s architecture, where the :math:`i^{th}` row is set by</span>
<span class="sd">        neuron :math:`i` and represents its weights towards other neurons. These weights influence the ranking and incentive</span>
<span class="sd">        mechanisms within the network. Higher weights from a neuron towards another can imply greater trust or value</span>
<span class="sd">        placed on that neuron&#39;s contributions.</span>

<span class="sd">        Returns:</span>
<span class="sd">            NDArray: A tensor of inter-peer weights, where each element :math:`w_{ij}` represents the weight assigned by neuron :math:`i` to neuron :math:`j`. This matrix is fundamental to the network&#39;s functioning, influencing the distribution of incentives and the inter-neuronal dynamics.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">hotkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Represents a list of ``hotkeys`` for each neuron in the Bittensor network.</span>

<span class="sd">        Hotkeys are unique identifiers used by neurons for active participation in the network, such as sending and receiving information or</span>
<span class="sd">        transactions. They are akin to public keys in cryptographic systems and are essential for identifying and authenticating neurons within the network&#39;s operations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List[str]: A list of hotkeys, with each string representing the hotkey of a corresponding neuron.</span>

<span class="sd">            These keys are crucial for the network&#39;s security and integrity, ensuring proper identification and authorization of network participants.</span>

<span class="sd">        Note:</span>
<span class="sd">            While the `NeurIPS paper &lt;https://bittensor.com/pdfs/academia/NeurIPS_DAO_Workshop_2022_3_3.pdf&gt;`_ may not explicitly detail the concept of hotkeys, they are a fundamental  of decentralized networks for secure and authenticated interactions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">axon</span><span class="o">.</span><span class="n">hotkey</span> <span class="k">for</span> <span class="n">axon</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">axons</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">coldkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Contains a list of ``coldkeys`` for each neuron in the Bittensor network.</span>

<span class="sd">        Coldkeys are similar to hotkeys but are typically used for more secure, offline activities such as storing assets or offline signing of transactions. They are an important aspect of a neuron&#39;s security, providing an additional layer of protection for sensitive operations and assets.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List[str]: A list of coldkeys, each string representing the coldkey of a neuron. These keys play a vital role in the secure management of assets and sensitive operations within the network.</span>

<span class="sd">        Note:</span>
<span class="sd">            The concept of coldkeys, while not explicitly covered in the NeurIPS paper, is a standard practice in</span>
<span class="sd">            blockchain and decentralized networks for enhanced security and asset protection.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">axon</span><span class="o">.</span><span class="n">coldkey</span> <span class="k">for</span> <span class="n">axon</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">axons</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">addresses</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Provides a list of IP addresses for each neuron in the Bittensor network. These addresses are used for</span>
<span class="sd">        network communication, allowing neurons to connect, interact, and exchange information with each other.</span>
<span class="sd">        IP addresses are fundamental for the network&#39;s peer-to-peer communication infrastructure.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List[str]: A list of IP addresses, with each string representing the address of a neuron. These addresses enable the decentralized, distributed nature of the network, facilitating direct communication and data exchange among neurons.</span>

<span class="sd">        Note:</span>
<span class="sd">            While IP addresses are a basic aspect of network communication, specific details about their use in</span>
<span class="sd">            the Bittensor network may not be covered in the `NeurIPS paper &lt;https://bittensor.com/pdfs/academia/NeurIPS_DAO_Workshop_2022_3_3.pdf&gt;`_. They are, however, integral to the</span>
<span class="sd">            functioning of any distributed network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">axon</span><span class="o">.</span><span class="n">ip_str</span><span class="p">()</span> <span class="k">for</span> <span class="n">axon</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">axons</span><span class="p">]</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">netuid</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">network</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">settings</span><span class="o">.</span><span class="n">DEFAULT_NETWORK</span><span class="p">,</span>
        <span class="n">lite</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">sync</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">subtensor</span><span class="p">:</span> <span class="s2">&quot;Subtensor&quot;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a new instance of the metagraph object, setting up the basic structure and parameters based on the provided arguments.</span>
<span class="sd">        This method is the entry point for creating a metagraph object,</span>
<span class="sd">        which is a central component in representing the state of the Bittensor network.</span>

<span class="sd">        Args:</span>
<span class="sd">            netuid (int): The unique identifier for the network, distinguishing this instance of the metagraph within potentially multiple network configurations.</span>
<span class="sd">            network (str): The name of the network, which can indicate specific configurations or versions of the Bittensor network.</span>
<span class="sd">            lite (bool): A flag indicating whether to use a lite version of the metagraph. The lite version may contain less detailed information but can be quicker to initialize and sync.</span>
<span class="sd">            sync (bool): A flag indicating whether to synchronize the metagraph with the network upon initialization. Synchronization involves updating the metagraph&#39;s parameters to reflect the current state of the network.</span>

<span class="sd">        Example:</span>
<span class="sd">            Initializing a metagraph object for the Bittensor network with a specific network UID::</span>

<span class="sd">                metagraph = metagraph(netuid=123, network=&quot;finney&quot;, lite=True, sync=True)</span>

<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Provides a human-readable string representation of the metagraph object. This representation includes key identifiers and attributes of the metagraph, making it easier to quickly understand</span>
<span class="sd">        the state and configuration of the metagraph in a simple format.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: A string that succinctly represents the metagraph, including its network UID, the total number of neurons (n), the current block number, and the network&#39;s name. This format is particularly useful for logging, debugging, and displaying the metagraph in a concise manner.</span>

<span class="sd">        Example:</span>
<span class="sd">            When printing the metagraph object or using it in a string context, this method is automatically invoked::</span>

<span class="sd">                print(metagraph)  # Output: &quot;metagraph(netuid:1, n:100, block:500, network:finney)&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;metagraph(netuid:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">netuid</span><span class="si">}</span><span class="s2">, n:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">, block:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">, network:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Provides a detailed string representation of the metagraph object, intended for unambiguous understanding and debugging purposes. This method simply calls the :func:`__str__` method, ensuring</span>
<span class="sd">        consistency between the informal and formal string representations of the metagraph.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: The same string representation as provided by the :func:`__str__` method, detailing the metagraph&#39;s key attributes including network UID, number of neurons, block number, and network name.</span>

<span class="sd">        Example:</span>
<span class="sd">            The :func:`__repr__` output can be used in debugging to get a clear and concise description of the metagraph::</span>

<span class="sd">                metagraph_repr = repr(metagraph)</span>
<span class="sd">                print(metagraph_repr)  # Output mirrors that of __str__</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>

<div class="viewcode-block" id="MetagraphMixin.metadata">
<a class="viewcode-back" href="../../../autoapi/bittensor/core/metagraph/index.html#bittensor.core.metagraph.MetagraphMixin.metadata">[docs]</a>
    <span class="k">def</span> <span class="nf">metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieves the metadata of the metagraph, providing key information about the current state of the</span>
<span class="sd">        Bittensor network. This metadata includes details such as the network&#39;s unique identifier (``netuid``),</span>
<span class="sd">        the total number of neurons (``n``), the current block number, the network&#39;s name, and the version of</span>
<span class="sd">        the Bittensor network.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: A dictionary containing essential metadata about the metagraph, including:</span>

<span class="sd">            - ``netuid``: The unique identifier for the network.</span>
<span class="sd">            - ``n``: The total number of neurons in the network.</span>
<span class="sd">            - ``block``: The current block number in the network&#39;s blockchain.</span>
<span class="sd">            - ``network``: The name of the Bittensor network.</span>
<span class="sd">            - ``version``: The version number of the Bittensor software.</span>

<span class="sd">        Note:</span>
<span class="sd">            This metadata is crucial for understanding the current state and configuration of the network, as well as for tracking its evolution over time.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;netuid&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">netuid</span><span class="p">,</span>
            <span class="s2">&quot;n&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="s2">&quot;block&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="s2">&quot;network&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span>
            <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="n">settings</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
        <span class="p">}</span></div>


<div class="viewcode-block" id="MetagraphMixin.state_dict">
<a class="viewcode-back" href="../../../autoapi/bittensor/core/metagraph/index.html#bittensor.core.metagraph.MetagraphMixin.state_dict">[docs]</a>
    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;netuid&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">netuid</span><span class="p">,</span>
            <span class="s2">&quot;network&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span>
            <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">version</span><span class="p">,</span>
            <span class="s2">&quot;n&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span>
            <span class="s2">&quot;block&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">,</span>
            <span class="s2">&quot;stake&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">stake</span><span class="p">,</span>
            <span class="s2">&quot;total_stake&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_stake</span><span class="p">,</span>
            <span class="s2">&quot;ranks&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ranks</span><span class="p">,</span>
            <span class="s2">&quot;trust&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trust</span><span class="p">,</span>
            <span class="s2">&quot;consensus&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">consensus</span><span class="p">,</span>
            <span class="s2">&quot;validator_trust&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">validator_trust</span><span class="p">,</span>
            <span class="s2">&quot;incentive&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">incentive</span><span class="p">,</span>
            <span class="s2">&quot;emission&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">emission</span><span class="p">,</span>
            <span class="s2">&quot;dividends&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dividends</span><span class="p">,</span>
            <span class="s2">&quot;active&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">active</span><span class="p">,</span>
            <span class="s2">&quot;last_update&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_update</span><span class="p">,</span>
            <span class="s2">&quot;validator_permit&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">validator_permit</span><span class="p">,</span>
            <span class="s2">&quot;weights&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
            <span class="s2">&quot;bonds&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bonds</span><span class="p">,</span>
            <span class="s2">&quot;uids&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">uids</span><span class="p">,</span>
            <span class="s2">&quot;axons&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">axons</span><span class="p">,</span>
            <span class="s2">&quot;neurons&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">,</span>
        <span class="p">}</span></div>


    <span class="k">def</span> <span class="nf">sync</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">block</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">lite</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">subtensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;Subtensor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Synchronizes the metagraph with the Bittensor network&#39;s current state. It updates the metagraph&#39;s attributes to reflect the latest data from the network, ensuring the metagraph represents the most current state of the network.</span>

<span class="sd">        Args:</span>
<span class="sd">            block (Optional[int]): A specific block number to synchronize with. If None, the metagraph syncs with the latest block. This allows for historical analysis or specific state examination of the network.</span>
<span class="sd">            lite (bool): If True, a lite version of the metagraph is used for quicker synchronization. This is beneficial when full detail is not necessary, allowing for reduced computational and time overhead.</span>
<span class="sd">            subtensor (Optional[bittensor.core.subtensor.Subtensor]): An instance of the subtensor class from Bittensor, providing an interface to the underlying blockchain data. If provided, this instance is used for data retrieval during synchronization.</span>

<span class="sd">        Example:</span>
<span class="sd">            Sync the metagraph with the latest block from the subtensor, using the lite version for efficiency::</span>

<span class="sd">                from bittensor.core.subtensor import Subtensor</span>

<span class="sd">                subtensor = Subtensor()</span>
<span class="sd">                metagraph.sync(subtensor=subtensor)</span>

<span class="sd">            Sync with a specific block number for detailed analysis::</span>

<span class="sd">                from bittensor.core.subtensor import Subtensor</span>

<span class="sd">                subtensor = Subtensor()</span>
<span class="sd">                metagraph.sync(block=12345, lite=False, subtensor=subtensor)</span>

<span class="sd">        NOTE:</span>
<span class="sd">            If attempting to access data beyond the previous 300 blocks, you **must** use the ``archive`` network for subtensor. Light nodes are configured only to store the previous 300 blocks if connecting to finney or test networks.</span>

<span class="sd">            For example::</span>

<span class="sd">                from bittensor.core.subtensor import Subtensor</span>

<span class="sd">                subtensor = Subtensor(network=&#39;archive&#39;)</span>
<span class="sd">                current_block = subtensor.get_current_block()</span>
<span class="sd">                history_block = current_block - 1200</span>

<span class="sd">                metagraph.sync(block=history_block, lite=False, subtensor=subtensor)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Initialize subtensor</span>
        <span class="n">subtensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_subtensor</span><span class="p">(</span><span class="n">subtensor</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">subtensor</span><span class="o">.</span><span class="n">chain_endpoint</span> <span class="o">!=</span> <span class="n">settings</span><span class="o">.</span><span class="n">ARCHIVE_ENTRYPOINT</span>
            <span class="ow">or</span> <span class="n">subtensor</span><span class="o">.</span><span class="n">network</span> <span class="o">!=</span> <span class="s2">&quot;archive&quot;</span>
        <span class="p">):</span>
            <span class="n">cur_block</span> <span class="o">=</span> <span class="n">subtensor</span><span class="o">.</span><span class="n">get_current_block</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">block</span> <span class="ow">and</span> <span class="n">block</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">cur_block</span> <span class="o">-</span> <span class="mi">300</span><span class="p">):</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Attempting to sync longer than 300 blocks ago on a non-archive node. Please use the &#39;archive&#39; &quot;</span>
                    <span class="s2">&quot;network for subtensor and retry.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Assign neurons based on &#39;lite&#39; flag</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assign_neurons</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">lite</span><span class="p">,</span> <span class="n">subtensor</span><span class="p">)</span>

        <span class="c1"># Set attributes for metagraph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_metagraph_attributes</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">subtensor</span><span class="p">)</span>

        <span class="c1"># If not a &#39;lite&#39; version, compute and set weights and bonds for each neuron</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">lite</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_weights_and_bonds</span><span class="p">(</span><span class="n">subtensor</span><span class="o">=</span><span class="n">subtensor</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_initialize_subtensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">subtensor</span><span class="p">:</span> <span class="s2">&quot;Subtensor&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the subtensor to be used for syncing the metagraph.</span>

<span class="sd">        This method ensures that a subtensor instance is available and properly set up for data retrieval during the synchronization process.</span>

<span class="sd">        If no subtensor is provided, this method is responsible for creating a new instance of the subtensor, configured according to the current network settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            subtensor (bittensor.core.subtensor.Subtensor): The subtensor instance provided for initialization. If ``None``, a new subtensor instance is created using the current network configuration.</span>

<span class="sd">        Returns:</span>
<span class="sd">            subtensor (bittensor.core.subtensor.Subtensor): The initialized subtensor instance, ready to be used for syncing the metagraph.</span>

<span class="sd">        Internal Usage:</span>
<span class="sd">            Used internally during the sync process to ensure a valid subtensor instance is available::</span>

<span class="sd">                subtensor = self._initialize_subtensor(subtensor)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">subtensor</span> <span class="ow">and</span> <span class="n">subtensor</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">subtensor</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">subtensor</span> <span class="o">=</span> <span class="n">subtensor</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">subtensor</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">subtensor</span><span class="p">:</span>
            <span class="n">subtensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">subtensor</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">subtensor</span><span class="p">:</span>
            <span class="c1"># TODO: Check and test the initialization of the new subtensor</span>
            <span class="c1"># Lazy import due to circular import (subtensor -&gt; metagraph, metagraph -&gt; subtensor)</span>
            <span class="kn">from</span> <span class="nn">bittensor.core.subtensor</span> <span class="kn">import</span> <span class="n">Subtensor</span>

            <span class="n">subtensor</span> <span class="o">=</span> <span class="n">Subtensor</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">chain_endpoint</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">subtensor</span> <span class="o">=</span> <span class="n">subtensor</span>
        <span class="k">return</span> <span class="n">subtensor</span>

    <span class="k">def</span> <span class="nf">_assign_neurons</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">lite</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">subtensor</span><span class="p">:</span> <span class="s2">&quot;Subtensor&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Assigns neurons to the metagraph based on the provided block number and the lite flag.</span>

<span class="sd">        This method is responsible for fetching and setting the neuron data in the metagraph, which includes neuron attributes like UID, stake, trust, and other relevant information.</span>

<span class="sd">        Args:</span>
<span class="sd">            block (int): The block number for which the neuron data needs to be fetched. If ``None``, the latest block data is used.</span>
<span class="sd">            lite (bool): A boolean flag indicating whether to use a lite version of the neuron data. The lite version typically includes essential information and is quicker to fetch and process.</span>
<span class="sd">            subtensor (bittensor.core.subtensor.Subtensor): The subtensor instance used for fetching neuron data from the network.</span>

<span class="sd">        Internal Usage:</span>
<span class="sd">            Used internally during the sync process to fetch and set neuron data::</span>

<span class="sd">                from bittensor.core.subtensor import Subtensor</span>

<span class="sd">                block = 12345</span>
<span class="sd">                lite = False</span>
<span class="sd">                subtensor = Subtensor()</span>
<span class="sd">                self._assign_neurons(block, lite, subtensor)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">lite</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span> <span class="o">=</span> <span class="n">subtensor</span><span class="o">.</span><span class="n">neurons_lite</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block</span><span class="p">,</span> <span class="n">netuid</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">netuid</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span> <span class="o">=</span> <span class="n">subtensor</span><span class="o">.</span><span class="n">neurons</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block</span><span class="p">,</span> <span class="n">netuid</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">netuid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lite</span> <span class="o">=</span> <span class="n">lite</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_create_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a numpy array with the given data and data type. This method is a utility function used internally to encapsulate data into a np.array, making it compatible with the metagraph&#39;s numpy model structure.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: The data to be included in the tensor. This could be any numeric data, like stakes, ranks, etc.</span>
<span class="sd">            dtype: The data type for the tensor, typically a numpy data type like ``np.float32`` or ``np.int64``.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tensor parameter encapsulating the provided data.</span>

<span class="sd">        Internal Usage:</span>
<span class="sd">            Used internally to create tensor parameters for various metagraph attributes::</span>

<span class="sd">                self.stake = self._create_tensor(neuron_stakes, dtype=np.float32)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: Check and test the creation of tensor</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">use_torch</span><span class="p">()</span>
            <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_weights_and_bonds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">subtensor</span><span class="p">:</span> <span class="s2">&quot;Optional[Subtensor]&quot;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes and sets the weights and bonds for each neuron in the metagraph. This method is responsible for processing the raw weight and bond data obtained from the network and converting it into a structured format suitable for the metagraph model.</span>

<span class="sd">        Args:</span>
<span class="sd">            subtensor: The subtensor instance used for fetching weights and bonds data. If ``None``, the weights and bonds are not updated.</span>

<span class="sd">        Internal Usage:</span>
<span class="sd">            Used internally during the sync process to update the weights and bonds of the neurons::</span>

<span class="sd">                self._set_weights_and_bonds(subtensor=subtensor)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: Check and test the computation of weights and bonds</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">netuid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_root_weights</span><span class="p">(</span>
                <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">weights</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span>
                <span class="s2">&quot;weights&quot;</span><span class="p">,</span>
                <span class="n">subtensor</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_weights_or_bonds</span><span class="p">(</span>
                <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">weights</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="s2">&quot;weights&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bonds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_weights_or_bonds</span><span class="p">(</span>
                <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">bonds</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="s2">&quot;bonds&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_process_weights_or_bonds</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">attribute</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Processes the raw weights or bonds data and converts it into a structured tensor format. This method handles the transformation of neuron connection data (``weights`` or ``bonds``) from a list or other unstructured format into a tensor that can be utilized within the metagraph model.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: The raw weights or bonds data to be processed. This data typically comes from the subtensor.</span>
<span class="sd">            attribute: A string indicating whether the data is ``weights`` or ``bonds``, which determines the specific processing steps to be applied.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tensor parameter encapsulating the processed weights or bonds data.</span>

<span class="sd">        Internal Usage:</span>
<span class="sd">            Used internally to process and set weights or bonds for the neurons::</span>

<span class="sd">                self.weights = self._process_weights_or_bonds(raw_weights_data, &quot;weights&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data_array</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">use_torch</span><span class="p">():</span>
                    <span class="n">data_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">)))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">data_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">uids</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">item</span><span class="p">)</span>
                <span class="c1"># TODO: Validate and test the conversion of uids and values to tensor</span>
                <span class="k">if</span> <span class="n">attribute</span> <span class="o">==</span> <span class="s2">&quot;weights&quot;</span><span class="p">:</span>
                    <span class="n">data_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">convert_weight_uids_and_vals_to_tensor</span><span class="p">(</span>
                            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">),</span>
                            <span class="nb">list</span><span class="p">(</span><span class="n">uids</span><span class="p">),</span>
                            <span class="nb">list</span><span class="p">(</span><span class="n">values</span><span class="p">),</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">data_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">convert_bond_uids_and_vals_to_tensor</span><span class="p">(</span>
                            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">),</span> <span class="nb">list</span><span class="p">(</span><span class="n">uids</span><span class="p">),</span> <span class="nb">list</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
                        <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                    <span class="p">)</span>
        <span class="n">tensor_param</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">,</span> <span class="n">NDArray</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">data_array</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_array</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">use_torch</span><span class="p">()</span>
            <span class="k">else</span> <span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">data_array</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_array</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_array</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Empty </span><span class="si">{</span><span class="n">attribute</span><span class="si">}</span><span class="s2">_array on metagraph.sync(). The &#39;</span><span class="si">{</span><span class="n">attribute</span><span class="si">}</span><span class="s2">&#39; tensor is empty.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_param</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_set_metagraph_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">subtensor</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">_process_root_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">attribute</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">subtensor</span><span class="p">:</span> <span class="s2">&quot;Subtensor&quot;</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Specifically processes the root weights data for the metagraph. This method is similar to :func:`_process_weights_or_bonds` but is tailored for processing root weights, which have a different structure and significance in the network.</span>

<span class="sd">        Args:</span>
<span class="sd">            data (list): The raw root weights data to be processed.</span>
<span class="sd">            attribute (str): A string indicating the attribute type, here it&#39;s typically ``weights``.</span>
<span class="sd">            subtensor (bittensor.core.subtensor.Subtensor): The subtensor instance used for additional data and context needed in processing.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tensor parameter encapsulating the processed root weights data.</span>

<span class="sd">        Internal Usage:</span>
<span class="sd">            Used internally to process and set root weights for the metagraph::</span>

<span class="sd">                self.root_weights = self._process_root_weights(raw_root_weights_data, &quot;weights&quot;, subtensor)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data_array</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">n_subnets</span> <span class="o">=</span> <span class="n">subtensor</span><span class="o">.</span><span class="n">get_total_subnets</span><span class="p">()</span> <span class="ow">or</span> <span class="mi">0</span>
        <span class="n">subnets</span> <span class="o">=</span> <span class="n">subtensor</span><span class="o">.</span><span class="n">get_subnets</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">use_torch</span><span class="p">():</span>
                    <span class="n">data_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_subnets</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">data_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_subnets</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">uids</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">item</span><span class="p">)</span>
                <span class="c1"># TODO: Validate and test the conversion of uids and values to tensor</span>
                <span class="n">data_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">convert_root_weight_uids_and_vals_to_tensor</span><span class="p">(</span>
                        <span class="n">n_subnets</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">uids</span><span class="p">),</span> <span class="nb">list</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">subnets</span>
                    <span class="p">)</span>
                <span class="p">)</span>

        <span class="n">tensor_param</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="s2">&quot;torch.nn.Parameter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">data_array</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_array</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">use_torch</span><span class="p">()</span>
            <span class="k">else</span> <span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">data_array</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_array</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_array</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Empty </span><span class="si">{</span><span class="n">attribute</span><span class="si">}</span><span class="s2">_array on metagraph.sync(). The &#39;</span><span class="si">{</span><span class="n">attribute</span><span class="si">}</span><span class="s2">&#39; tensor is empty.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_param</span>

<div class="viewcode-block" id="MetagraphMixin.save">
<a class="viewcode-back" href="../../../autoapi/bittensor/core/metagraph/index.html#bittensor.core.metagraph.MetagraphMixin.save">[docs]</a>
    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Metagraph&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves the current state of the metagraph to a file on disk. This function is crucial for persisting the current state of the network&#39;s metagraph, which can later be reloaded or analyzed. The save operation includes all neuron attributes and parameters, ensuring a complete snapshot of the metagraph&#39;s state.</span>

<span class="sd">        Returns:</span>
<span class="sd">            metagraph (bittensor.core.metagraph.Metagraph): The metagraph instance after saving its state.</span>

<span class="sd">        Example:</span>
<span class="sd">            Save the current state of the metagraph to the default directory::</span>

<span class="sd">                metagraph.save()</span>

<span class="sd">            The saved state can later be loaded to restore or analyze the metagraph&#39;s state at this point.</span>

<span class="sd">            If using the default save path::</span>

<span class="sd">                metagraph.load()</span>

<span class="sd">            If using a custom save path::</span>

<span class="sd">                metagraph.load_from_path(dir_path)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">save_directory</span> <span class="o">=</span> <span class="n">get_save_dir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">netuid</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_torch</span><span class="p">():</span>
            <span class="n">graph_filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">/block-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">.pt&quot;</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;axons&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axons</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;neurons&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">graph_filename</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">graph_filename</span><span class="p">)</span>  <span class="c1"># verifies that the file can be loaded correctly</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">graph_filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">/block-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">.pt&quot;</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">graph_filename</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">graph_file</span><span class="p">:</span>
                <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">graph_file</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>


<div class="viewcode-block" id="MetagraphMixin.load">
<a class="viewcode-back" href="../../../autoapi/bittensor/core/metagraph/index.html#bittensor.core.metagraph.MetagraphMixin.load">[docs]</a>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads the state of the metagraph from the default save directory. This method is instrumental for restoring the metagraph to its last saved state. It automatically identifies the save directory based on the ``network`` and ``netuid`` properties of the metagraph, locates the latest block file in that directory, and loads all metagraph parameters from it.</span>

<span class="sd">        This functionality is particularly beneficial when continuity in the state of the metagraph is necessary</span>
<span class="sd">        across different runtime sessions, or after a restart of the system. It ensures that the metagraph reflects</span>
<span class="sd">        the exact state it was in at the last save point, maintaining consistency in the network&#39;s representation.</span>

<span class="sd">        The method delegates to ``load_from_path``, supplying it with the directory path constructed from the metagraph&#39;s current ``network`` and ``netuid`` properties. This abstraction simplifies the process of loading the metagraph&#39;s state for the user, requiring no direct path specifications.</span>

<span class="sd">        Returns:</span>
<span class="sd">            metagraph (bittensor.core.metagraph.Metagraph): The metagraph instance after loading its state from the default directory.</span>

<span class="sd">        Example:</span>
<span class="sd">            Load the metagraph state from the last saved snapshot in the default directory::</span>

<span class="sd">                metagraph.load()</span>

<span class="sd">            After this operation, the metagraph&#39;s parameters and neuron data are restored to their state at the time of the last save in the default directory.</span>

<span class="sd">        Note:</span>
<span class="sd">            The default save directory is determined based on the metagraph&#39;s ``network`` and ``netuid`` attributes. It is important to ensure that these attributes are set correctly and that the default save directory contains the appropriate state files for the metagraph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_from_path</span><span class="p">(</span><span class="n">get_save_dir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">netuid</span><span class="p">))</span></div>


<div class="viewcode-block" id="MetagraphMixin.load_from_path">
<a class="viewcode-back" href="../../../autoapi/bittensor/core/metagraph/index.html#bittensor.core.metagraph.MetagraphMixin.load_from_path">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">load_from_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dir_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Metagraph&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads the state of the metagraph from a specified directory path. This method is crucial for restoring the metagraph to a specific state based on saved data. It locates the latest block file in the given</span>
<span class="sd">        directory and loads all metagraph parameters from it. This is particularly useful for analyses that require historical states of the network or for restoring previous states of the metagraph in different</span>
<span class="sd">        execution environments.</span>

<span class="sd">        The method first identifies the latest block file in the specified directory, then loads the metagraph state including neuron attributes and parameters from this file. This ensures that the metagraph is accurately reconstituted to reflect the network state at the time of the saved block.</span>

<span class="sd">        Args:</span>
<span class="sd">            dir_path (str): The directory path where the metagraph&#39;s state files are stored. This path should contain one or more saved state files, typically named in a format that includes the block number.</span>

<span class="sd">        Returns:</span>
<span class="sd">            metagraph (bittensor.core.metagraph.Metagraph): The metagraph instance after loading its state from the specified directory path.</span>

<span class="sd">        Example:</span>
<span class="sd">            Load the metagraph state from a specific directory::</span>

<span class="sd">                dir_path = &quot;/path/to/saved/metagraph/states&quot;</span>
<span class="sd">                metagraph.load_from_path(dir_path)</span>

<span class="sd">            The metagraph is now restored to the state it was in at the time of the latest saved block in the specified directory.</span>

<span class="sd">        Note:</span>
<span class="sd">            This method assumes that the state files in the specified directory are correctly formatted and</span>
<span class="sd">            contain valid data for the metagraph. It is essential to ensure that the directory path and the</span>
<span class="sd">            state files within it are accurate and consistent with the expected metagraph structure.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>


    <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span>
        <span class="n">new_instance</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
        <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span> <span class="o">=</span> <span class="n">new_instance</span>

        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;subtensor&quot;</span><span class="p">:</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">new_instance</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">new_instance</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">memo</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">new_instance</span>

    <span class="k">def</span> <span class="nf">__copy__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span>
        <span class="n">new_instance</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;subtensor&quot;</span><span class="p">:</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">new_instance</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">new_instance</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_instance</span></div>



<span class="n">BaseClass</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;torch.nn.Module&quot;</span><span class="p">,</span> <span class="nb">object</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="k">if</span> <span class="n">use_torch</span><span class="p">()</span> <span class="k">else</span> <span class="nb">object</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Base class that extends :class:`torch.nn.Module` if PyTorch is used; otherwise, it defaults to object.</span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="k">class</span> <span class="nc">TorchMetaGraph</span><span class="p">(</span><span class="n">MetagraphMixin</span><span class="p">,</span> <span class="n">BaseClass</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">netuid</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">network</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">settings</span><span class="o">.</span><span class="n">DEFAULT_NETWORK</span><span class="p">,</span>
        <span class="n">lite</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">sync</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">subtensor</span><span class="p">:</span> <span class="s2">&quot;Subtensor&quot;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a new instance of the metagraph object, setting up the basic structure and parameters based on the provided arguments.</span>
<span class="sd">        This class requires Torch to be installed.</span>
<span class="sd">        This method is the entry point for creating a metagraph object, which is a central component in representing the state of the Bittensor network.</span>

<span class="sd">        Args:</span>
<span class="sd">            netuid (int): The unique identifier for the network, distinguishing this instance of the metagraph within potentially multiple network configurations.</span>
<span class="sd">            network (str): The name of the network, which can indicate specific configurations or versions of the Bittensor network.</span>
<span class="sd">            lite (bool): A flag indicating whether to use a lite version of the metagraph. The lite version may contain less detailed information but can be quicker to initialize and sync.</span>
<span class="sd">            sync (bool): A flag indicating whether to synchronize the metagraph with the network upon initialization. Synchronization involves updating the metagraph&#39;s parameters to reflect the current state of the network.</span>

<span class="sd">        Example:</span>
<span class="sd">            Initializing a metagraph object for the Bittensor network with a specific network UID::</span>

<span class="sd">                from bittensor.core.metagraph import Metagraph</span>

<span class="sd">                metagraph = Metagraph(netuid=123, network=&quot;finney&quot;, lite=True, sync=True)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">MetagraphMixin</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">netuid</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">lite</span><span class="p">,</span> <span class="n">sync</span><span class="p">,</span> <span class="n">subtensor</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">netuid</span> <span class="o">=</span> <span class="n">netuid</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">chain_endpoint</span> <span class="o">=</span> <span class="n">determine_chain_endpoint_and_network</span><span class="p">(</span>
            <span class="n">network</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">version</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">settings</span><span class="o">.</span><span class="n">version_as_int</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stake</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_stake</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ranks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trust</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">consensus</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_trust</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">incentive</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emission</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dividends</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_update</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_permit</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bonds</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axons</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">AxonInfo</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subtensor</span> <span class="o">=</span> <span class="n">subtensor</span>
        <span class="k">if</span> <span class="n">sync</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sync</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lite</span><span class="o">=</span><span class="n">lite</span><span class="p">,</span> <span class="n">subtensor</span><span class="o">=</span><span class="n">subtensor</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_metagraph_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">subtensor</span><span class="p">:</span> <span class="s2">&quot;Subtensor&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets various attributes of the metagraph based on the latest network data fetched from the subtensor.</span>

<span class="sd">        This method updates parameters like the number of neurons, block number, stakes, trusts, ranks, and other neuron-specific information.</span>

<span class="sd">        Args:</span>
<span class="sd">            block (int): The block number for which the metagraph attributes need to be set. If ``None``, the latest block data is used.</span>
<span class="sd">            subtensor (bittensor.core.subtensor.Subtensor): The subtensor instance used for fetching the latest network data.</span>

<span class="sd">        Internal Usage:</span>
<span class="sd">            Used internally during the sync process to update the metagraph&#39;s attributes::</span>

<span class="sd">                from bittensor.core.subtensor import Subtensor</span>

<span class="sd">                subtensor = Subtensor()</span>
<span class="sd">                block = subtensor.get_current_block()</span>

<span class="sd">                self._set_metagraph_attributes(block, subtensor)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">version</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">([</span><span class="n">settings</span><span class="o">.</span><span class="n">version_as_int</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="n">block</span> <span class="k">if</span> <span class="n">block</span> <span class="k">else</span> <span class="n">subtensor</span><span class="o">.</span><span class="n">block</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">uid</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trust</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">trust</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">consensus</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">consensus</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">incentive</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">incentive</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dividends</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">dividends</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ranks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">rank</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emission</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">emission</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">active</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">last_update</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_permit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">validator_permit</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_trust</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">validator_trust</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_stake</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">total_stake</span><span class="o">.</span><span class="n">tao</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stake</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">stake</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axons</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span><span class="o">.</span><span class="n">axon_info</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">load_from_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dir_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Metagraph&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads the metagraph state from a specified directory path.</span>

<span class="sd">        Args:</span>
<span class="sd">            dir_path (str): The directory path where the state file is located.</span>

<span class="sd">        Returns:</span>
<span class="sd">            metagraph (bittensor.core.metagraph.Metagraph): The current metagraph instance with the loaded state.</span>

<span class="sd">        Example::</span>

<span class="sd">            from bittensor.core.metagraph import Metagraph</span>

<span class="sd">            netuid = 1</span>
<span class="sd">            metagraph = Metagraph(netuid=netuid)</span>

<span class="sd">            metagraph.load_from_path(&quot;/path/to/dir&quot;)</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">graph_file</span> <span class="o">=</span> <span class="n">latest_block_path</span><span class="p">(</span><span class="n">dir_path</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">graph_file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;n&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;block&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;uids&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stake</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;stake&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_stake</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;total_stake&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ranks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;ranks&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trust</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;trust&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">consensus</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;consensus&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_trust</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;validator_trust&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">incentive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;incentive&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emission</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;emission&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dividends</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;dividends&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;active&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_update</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;last_update&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_permit</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;validator_permit&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;uids&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axons</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;axons&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;neurons&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="s2">&quot;weights&quot;</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;weights&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;bonds&quot;</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bonds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;bonds&quot;</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>


<div class="viewcode-block" id="NonTorchMetagraph">
<a class="viewcode-back" href="../../../autoapi/bittensor/core/metagraph/index.html#bittensor.core.metagraph.NonTorchMetagraph">[docs]</a>
<span class="k">class</span> <span class="nc">NonTorchMetagraph</span><span class="p">(</span><span class="n">MetagraphMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">netuid</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">network</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">settings</span><span class="o">.</span><span class="n">DEFAULT_NETWORK</span><span class="p">,</span>
        <span class="n">lite</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">sync</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">subtensor</span><span class="p">:</span> <span class="s2">&quot;Subtensor&quot;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a new instance of the metagraph object, setting up the basic structure and parameters based on the provided arguments.</span>
<span class="sd">        This class doesn&#39;t require installed Torch.</span>
<span class="sd">        This method is the entry point for creating a metagraph object, which is a central component in representing the state of the Bittensor network.</span>

<span class="sd">        Args:</span>
<span class="sd">            netuid (int): The unique identifier for the network, distinguishing this instance of the metagraph within potentially multiple network configurations.</span>
<span class="sd">            network (str): The name of the network, which can indicate specific configurations or versions of the Bittensor network.</span>
<span class="sd">            lite (bool): A flag indicating whether to use a lite version of the metagraph. The lite version may contain less detailed information but can be quicker to initialize and sync.</span>
<span class="sd">            sync (bool): A flag indicating whether to synchronize the metagraph with the network upon initialization. Synchronization involves updating the metagraph&#39;s parameters to reflect the current state of the network.</span>

<span class="sd">        Example:</span>
<span class="sd">            Initializing a metagraph object for the Bittensor network with a specific network UID::</span>

<span class="sd">                from bittensor.core.metagraph import Metagraph</span>

<span class="sd">                metagraph = Metagraph(netuid=123, network=&quot;finney&quot;, lite=True, sync=True)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># super(metagraph, self).__init__()</span>
        <span class="n">MetagraphMixin</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">netuid</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">lite</span><span class="p">,</span> <span class="n">sync</span><span class="p">,</span> <span class="n">subtensor</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">netuid</span> <span class="o">=</span> <span class="n">netuid</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">chain_endpoint</span> <span class="o">=</span> <span class="n">determine_chain_endpoint_and_network</span><span class="p">(</span>
            <span class="n">network</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">version</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">settings</span><span class="o">.</span><span class="n">version_as_int</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">),)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_stake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ranks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trust</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">consensus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_trust</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">incentive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emission</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dividends</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_permit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bonds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axons</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">AxonInfo</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subtensor</span> <span class="o">=</span> <span class="n">subtensor</span>
        <span class="k">if</span> <span class="n">sync</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sync</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lite</span><span class="o">=</span><span class="n">lite</span><span class="p">,</span> <span class="n">subtensor</span><span class="o">=</span><span class="n">subtensor</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_metagraph_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">subtensor</span><span class="p">:</span> <span class="s2">&quot;Subtensor&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets various attributes of the metagraph based on the latest network data fetched from the subtensor.</span>

<span class="sd">        This method updates parameters like the number of neurons, block number, stakes, trusts, ranks, and other neuron-specific information.</span>

<span class="sd">        Args:</span>
<span class="sd">            block (int): The block number for which the metagraph attributes need to be set. If ``None``, the latest block data is used.</span>
<span class="sd">            subtensor (bittensor.core.subtensor.Subtensor): The subtensor instance used for fetching the latest network data.</span>

<span class="sd">        Internal Usage:</span>
<span class="sd">            Used internally during the sync process to update the metagraph&#39;s attributes::</span>

<span class="sd">                self._set_metagraph_attributes(block, subtensor)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: Check and test the setting of each attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">version</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">([</span><span class="n">settings</span><span class="o">.</span><span class="n">version_as_int</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="n">block</span> <span class="k">if</span> <span class="n">block</span> <span class="k">else</span> <span class="n">subtensor</span><span class="o">.</span><span class="n">block</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">uid</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trust</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">trust</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">consensus</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">consensus</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">incentive</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">incentive</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dividends</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">dividends</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ranks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">rank</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emission</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">emission</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">active</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">last_update</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_permit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">validator_permit</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_trust</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">validator_trust</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_stake</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">total_stake</span><span class="o">.</span><span class="n">tao</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stake</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neuron</span><span class="o">.</span><span class="n">stake</span> <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axons</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span><span class="o">.</span><span class="n">axon_info</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">]</span>

<div class="viewcode-block" id="NonTorchMetagraph.load_from_path">
<a class="viewcode-back" href="../../../autoapi/bittensor/core/metagraph/index.html#bittensor.core.metagraph.NonTorchMetagraph.load_from_path">[docs]</a>
    <span class="k">def</span> <span class="nf">load_from_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dir_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Metagraph&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads the state of the Metagraph from a specified directory path.</span>

<span class="sd">        Args:</span>
<span class="sd">            dir_path (str): The directory path where the metagraph&#39;s state file is located.</span>

<span class="sd">        Returns:</span>
<span class="sd">            metagraph (:func:`bittensor.core.metagraph.Metagraph`): An instance of the Metagraph with the state loaded from the file.</span>

<span class="sd">        Raises:</span>
<span class="sd">            pickle.UnpicklingError: If there is an error unpickling the state file.</span>
<span class="sd">            RuntimeError: If there is an error loading the state file using PyTorch.</span>
<span class="sd">            ImportError: If there is an error importing PyTorch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">graph_filename</span> <span class="o">=</span> <span class="n">latest_block_path</span><span class="p">(</span><span class="n">dir_path</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">graph_filename</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">graph_file</span><span class="p">:</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">graph_file</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">pickle</span><span class="o">.</span><span class="n">UnpicklingError</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Unable to load file. Attempting to restore metagraph using torch.&quot;</span>
            <span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;:warning: This functionality exists to load metagraph state from legacy saves, but will not be supported in the future.&quot;</span>
            <span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">real_torch</span>

                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">real_torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">graph_filename</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">METAGRAPH_STATE_DICT_NDARRAY_KEYS</span><span class="p">:</span>
                    <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">del</span> <span class="n">real_torch</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">RuntimeError</span><span class="p">,</span> <span class="ne">ImportError</span><span class="p">):</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Unable to load file. It may be corrupted.&quot;</span><span class="p">)</span>
                <span class="k">raise</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;n&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;block&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uids</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;uids&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stake</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;stake&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_stake</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;total_stake&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ranks</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;ranks&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trust</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;trust&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">consensus</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;consensus&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_trust</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;validator_trust&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">incentive</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;incentive&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emission</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;emission&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dividends</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;dividends&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;active&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_update</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;last_update&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validator_permit</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;validator_permit&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axons</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;axons&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;neurons&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="s2">&quot;weights&quot;</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;weights&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="s2">&quot;bonds&quot;</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bonds</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;bonds&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span></div>
</div>



<span class="n">Metagraph</span> <span class="o">=</span> <span class="n">TorchMetaGraph</span> <span class="k">if</span> <span class="n">use_torch</span><span class="p">()</span> <span class="k">else</span> <span class="n">NonTorchMetagraph</span>
<span class="sd">&quot;&quot;&quot;Metagraph class that uses :class:`TorchMetaGraph` if PyTorch is available; otherwise, it falls back to :class:`NonTorchMetagraph`.</span>

<span class="sd">- **With PyTorch**: When `use_torch()` returns `True`, `Metagraph` is set to :class:`TorchMetaGraph`, which utilizes PyTorch functionalities.</span>
<span class="sd">- **Without PyTorch**: When `use_torch()` returns `False`, `Metagraph` is set to :class:`NonTorchMetagraph`, which does not rely on PyTorch.</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Opentensor Foundation
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, Opentensor Foundation.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>